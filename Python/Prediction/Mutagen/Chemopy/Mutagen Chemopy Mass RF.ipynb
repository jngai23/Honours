{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f438c417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#21/6/24 Random Forest Model Creation for Basak Descriptors of Mutagen Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Data import\n",
    "seed = 82\n",
    "\n",
    "datasets = []\n",
    "index = []\n",
    "directory = '/Users/james/Documents/Honours/Data/structdata/Mutagen/chemopy/'\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if 'csv' in filename:\n",
    "        pathname = directory + file\n",
    "        df = pd.read_csv(pathname)\n",
    "        df.drop(columns=['SMILES'], inplace=True)\n",
    "        df = df.dropna(axis=1)\n",
    "        datasets.append(df)\n",
    "        index.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b158900b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "mut_kappa.csv\n",
      "False\n",
      "mut_basak.csv\n",
      "False\n",
      "mut_connectivity.csv\n",
      "False\n",
      "mut_constitution.csv\n",
      "False\n",
      "mut_moe.csv\n",
      "False\n",
      "mut_charge.csv\n",
      "False\n",
      "mut_estate.csv\n",
      "False\n",
      "mut_molprop.csv\n",
      "False\n",
      "mut_topological.csv\n"
     ]
    }
   ],
   "source": [
    "loops = 0\n",
    "for df in datasets:\n",
    "    print(df.isnull().values.any())\n",
    "    print(index[loops])\n",
    "    loops = loops + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "51188e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate various metrics, outputs a list of various metrics with a consistent index\n",
    "def metriccalc(preds, ytrain):\n",
    "    correctcount = 0\n",
    "    fpcount = 0\n",
    "    tpcount = 0\n",
    "    tncount = 0\n",
    "    fncount = 0\n",
    "    testpos = 0\n",
    "    testneg = 0\n",
    "    \n",
    "    #loop through each item in the predictions, logging positives, negatives and tn/tp/fn/tp\n",
    "    iterations = 0\n",
    "    for value in preds:\n",
    "        testscore = ytrain[iterations]\n",
    "        if value == 1:\n",
    "            if testscore != 0:\n",
    "                testpos = testpos + 1\n",
    "                correctcount = correctcount + 1\n",
    "                tpcount = tpcount + 1\n",
    "            else:\n",
    "                fpcount = fpcount + 1\n",
    "                testneg = testneg + 1\n",
    "        else:\n",
    "            if testscore != 0:\n",
    "                testpos = testpos + 1\n",
    "                fncount = fncount + 1\n",
    "            else:\n",
    "                testneg = testneg + 1\n",
    "                correctcount = correctcount + 1\n",
    "                tncount = tncount + 1\n",
    "\n",
    "        iterations = iterations + 1\n",
    "    \n",
    "    #calculate a wide swathe of metrics\n",
    "    netfn = fncount / (fncount + tncount)\n",
    "    nettn = tncount / (fncount + tncount)\n",
    "    netacc = correctcount / (fpcount + fncount + tpcount + tncount)\n",
    "    posacc = tpcount / testpos\n",
    "    negacc = tncount / testneg\n",
    "    netfp = fpcount / (fpcount + tpcount)\n",
    "    nettp = tpcount / (tpcount + fpcount)\n",
    "\n",
    "    fpr = fpcount / (fpcount + tncount)\n",
    "    tpr = tpcount / (tpcount + fncount)\n",
    "\n",
    "\n",
    "\n",
    "    f1 = (2 * tpcount) / ((2 * tpcount) + fpcount + fncount)\n",
    "\n",
    "\n",
    "    tp = tpcount\n",
    "    fp = fpcount\n",
    "    tn = tncount\n",
    "    fn = fncount\n",
    "\n",
    "    \n",
    "    temp = math.sqrt((fp + tn) * (tp + fp) * (tp + fn) * (tn + fn))\n",
    "    if temp == 0:\n",
    "        return [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    mcc = ((tp * tn) - (fp * fn)) / temp\n",
    "\n",
    "\n",
    "\n",
    "    temp = (( ( (tp + fp) * (fp + tn) ) + ( (tp + fn) * (fn + tn) ) ))\n",
    "    if temp == 0:\n",
    "        return 0\n",
    "    kapp =  ( 2 * ((tp * tn) - (fn * fp)) ) / temp\n",
    "    \n",
    "    metriclist = [testpos, testneg, fn, tn, tp, fp, netacc, posacc, negacc, fpr, tpr, f1, mcc, kapp]\n",
    "    \n",
    "    return metriclist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a3557f27",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mcc of 0.39195354248497805 with an mtry of 22\n",
      "for dataset mut_kappa.csv validation metrics of:\n",
      "positives in data 2093\n",
      "negatives in data 1650 \n",
      "\n",
      "fn count = 519\n",
      "tn count = 1052\n",
      "tp count = 1574\n",
      "fp count = 598 \n",
      "\n",
      "net accuracy = 0.7015762757146674\n",
      "positive accuracy = 0.7520305781175346\n",
      "negative accuracy = 0.6375757575757576 \n",
      "\n",
      "fpr = 0.3624242424242424\n",
      "tpr = 0.7520305781175346 \n",
      "\n",
      "f1 score = 0.7381008206330598\n",
      "mcc = 0.39195354248497805\n",
      "cohen Kappa = 0.391590509935894 \n",
      " \n",
      "\n",
      "best mcc of 0.4216751150672551 with an mtry of 21\n",
      "for dataset mut_basak.csv validation metrics of:\n",
      "positives in data 2093\n",
      "negatives in data 1650 \n",
      "\n",
      "fn count = 462\n",
      "tn count = 1052\n",
      "tp count = 1631\n",
      "fp count = 598 \n",
      "\n",
      "net accuracy = 0.7168047021106064\n",
      "positive accuracy = 0.7792642140468228\n",
      "negative accuracy = 0.6375757575757576 \n",
      "\n",
      "fpr = 0.3624242424242424\n",
      "tpr = 0.7792642140468228 \n",
      "\n",
      "f1 score = 0.7547431744562703\n",
      "mcc = 0.4216751150672551\n",
      "cohen Kappa = 0.4205080088779158 \n",
      " \n",
      "\n",
      "best mcc of 0.4984993664992035 with an mtry of 29\n",
      "for dataset mut_connectivity.csv validation metrics of:\n",
      "positives in data 2093\n",
      "negatives in data 1650 \n",
      "\n",
      "fn count = 413\n",
      "tn count = 1142\n",
      "tp count = 1680\n",
      "fp count = 508 \n",
      "\n",
      "net accuracy = 0.7539406892866685\n",
      "positive accuracy = 0.802675585284281\n",
      "negative accuracy = 0.6921212121212121 \n",
      "\n",
      "fpr = 0.30787878787878786\n",
      "tpr = 0.802675585284281 \n",
      "\n",
      "f1 score = 0.7848633496846531\n",
      "mcc = 0.4984993664992035\n",
      "cohen Kappa = 0.4978301673096799 \n",
      " \n",
      "\n",
      "best mcc of 0.5593329204713424 with an mtry of 27\n",
      "for dataset mut_constitution.csv validation metrics of:\n",
      "positives in data 2093\n",
      "negatives in data 1650 \n",
      "\n",
      "fn count = 341\n",
      "tn count = 1182\n",
      "tp count = 1752\n",
      "fp count = 468 \n",
      "\n",
      "net accuracy = 0.7838632113278119\n",
      "positive accuracy = 0.8370759675107501\n",
      "negative accuracy = 0.7163636363636363 \n",
      "\n",
      "fpr = 0.28363636363636363\n",
      "tpr = 0.8370759675107501 \n",
      "\n",
      "f1 score = 0.8124275446325063\n",
      "mcc = 0.5593329204713424\n",
      "cohen Kappa = 0.557984736898266 \n",
      " \n",
      "\n",
      "best mcc of 0.6091448642777267 with an mtry of 44\n",
      "for dataset mut_moe.csv validation metrics of:\n",
      "positives in data 2093\n",
      "negatives in data 1650 \n",
      "\n",
      "fn count = 345\n",
      "tn count = 1275\n",
      "tp count = 1748\n",
      "fp count = 375 \n",
      "\n",
      "net accuracy = 0.8076409297355063\n",
      "positive accuracy = 0.8351648351648352\n",
      "negative accuracy = 0.7727272727272727 \n",
      "\n",
      "fpr = 0.22727272727272727\n",
      "tpr = 0.8351648351648352 \n",
      "\n",
      "f1 score = 0.8292220113851992\n",
      "mcc = 0.6091448642777267\n",
      "cohen Kappa = 0.6090640462689361 \n",
      " \n",
      "\n",
      "best mcc of 0.4618908068046193 with an mtry of 16\n",
      "for dataset mut_charge.csv validation metrics of:\n",
      "positives in data 2093\n",
      "negatives in data 1650 \n",
      "\n",
      "fn count = 527\n",
      "tn count = 1180\n",
      "tp count = 1566\n",
      "fp count = 470 \n",
      "\n",
      "net accuracy = 0.7336361207587496\n",
      "positive accuracy = 0.7482083134257047\n",
      "negative accuracy = 0.7151515151515152 \n",
      "\n",
      "fpr = 0.28484848484848485\n",
      "tpr = 0.7482083134257047 \n",
      "\n",
      "f1 score = 0.758537176071688\n",
      "mcc = 0.4618908068046193\n",
      "cohen Kappa = 0.46167199762382555 \n",
      " \n",
      "\n",
      "best mcc of 0.47890125078704515 with an mtry of 45\n",
      "for dataset mut_estate.csv validation metrics of:\n",
      "positives in data 2093\n",
      "negatives in data 1650 \n",
      "\n",
      "fn count = 417\n",
      "tn count = 1111\n",
      "tp count = 1676\n",
      "fp count = 539 \n",
      "\n",
      "net accuracy = 0.7445899011488111\n",
      "positive accuracy = 0.800764452938366\n",
      "negative accuracy = 0.6733333333333333 \n",
      "\n",
      "fpr = 0.32666666666666666\n",
      "tpr = 0.800764452938366 \n",
      "\n",
      "f1 score = 0.7780872794800371\n",
      "mcc = 0.47890125078704515\n",
      "cohen Kappa = 0.47783682535772687 \n",
      " \n",
      "\n",
      "best mcc of 0.4736283889952646 with an mtry of 35\n",
      "for dataset mut_molprop.csv validation metrics of:\n",
      "positives in data 2093\n",
      "negatives in data 1650 \n",
      "\n",
      "fn count = 410\n",
      "tn count = 1095\n",
      "tp count = 1683\n",
      "fp count = 555 \n",
      "\n",
      "net accuracy = 0.742185412770505\n",
      "positive accuracy = 0.8041089345437171\n",
      "negative accuracy = 0.6636363636363637 \n",
      "\n",
      "fpr = 0.33636363636363636\n",
      "tpr = 0.8041089345437171 \n",
      "\n",
      "f1 score = 0.7771877164627107\n",
      "mcc = 0.4736283889952646\n",
      "cohen Kappa = 0.47213622177908754 \n",
      " \n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 28\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mtry \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m50\u001b[39m):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m#using mtry as the adjusted hyperparameter creates a series of random forests\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     rf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39mmtry, criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[1;32m     22\u001b[0m                             min_samples_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, min_samples_leaf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m     23\u001b[0m                             min_weight_fraction_leaf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqrt\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     24\u001b[0m                             max_leaf_nodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, min_impurity_decrease\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, \n\u001b[1;32m     25\u001b[0m                             bootstrap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, oob_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, n_jobs\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mseed, \n\u001b[1;32m     26\u001b[0m                             verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, warm_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ccp_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, max_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 28\u001b[0m     model\u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mfit(xtrain, ytrain)\n\u001b[1;32m     29\u001b[0m     preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(xtest)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m#calculate metric (mcc)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:348\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 348\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    349\u001b[0m     X, y, multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mDTYPE\n\u001b[1;32m    350\u001b[0m )\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:621\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    619\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    622\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1147\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1142\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1144\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1145\u001b[0m     )\n\u001b[0;32m-> 1147\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1148\u001b[0m     X,\n\u001b[1;32m   1149\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   1150\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[1;32m   1151\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1152\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[1;32m   1153\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m   1154\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[1;32m   1155\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[1;32m   1156\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[1;32m   1157\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[1;32m   1158\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[1;32m   1159\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m   1160\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1161\u001b[0m )\n\u001b[1;32m   1163\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1165\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    954\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    955\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    956\u001b[0m         )\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 959\u001b[0m         _assert_all_finite(\n\u001b[1;32m    960\u001b[0m             array,\n\u001b[1;32m    961\u001b[0m             input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    962\u001b[0m             estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    963\u001b[0m             allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    964\u001b[0m         )\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    125\u001b[0m     X,\n\u001b[1;32m    126\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[1;32m    127\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[1;32m    128\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[1;32m    129\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    130\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    131\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m     )\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "loops = 0\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "for targetdata in datasets:\n",
    "    trainset, testset = train_test_split(targetdata, test_size=0.5, random_state=seed)\n",
    "    xtrain = trainset.iloc[:, 1:]\n",
    "    ytrain = trainset.iloc[:, 0].values\n",
    "    xtest = testset.iloc[:, 1:]\n",
    "    ytest = testset.iloc[:, 0].values\n",
    "\n",
    "#    scaler = preprocessing.StandardScaler().fit(xtrain)\n",
    "#    xtrain = scaler.transform(xtrain)\n",
    "    \n",
    "#    scaler = preprocessing.StandardScaler().fit(xtest)\n",
    "#    xtest = scaler.transform(xtest)\n",
    "    \n",
    "    mcclist = []\n",
    "    maxval = 0\n",
    "    for mtry in range(1, 50):\n",
    "        #using mtry as the adjusted hyperparameter creates a series of random forests\n",
    "        rf = RandomForestClassifier(n_estimators=mtry, criterion='entropy', max_depth=None, \n",
    "                                min_samples_split=2, min_samples_leaf=1, \n",
    "                                min_weight_fraction_leaf=0.0, max_features='sqrt', \n",
    "                                max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                                bootstrap=True, oob_score=False, n_jobs= 4, random_state=seed, \n",
    "                                verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
    "\n",
    "        model= rf.fit(xtrain, ytrain)\n",
    "        preds = model.predict(xtest)\n",
    "        #calculate metric (mcc)\n",
    "        mcc = metriccalc(preds, ytest)[12]\n",
    "        mcclist.append(mcc)\n",
    "        if mcc > maxval:\n",
    "            maxval = mcc\n",
    "            bestmetrics = mtry\n",
    "    #store best model for the given fold and plot the metric vs mcc value\n",
    "    rf = RandomForestClassifier(n_estimators=bestmetrics, criterion='entropy', max_depth=None, \n",
    "                                min_samples_split=2, min_samples_leaf=1, \n",
    "                                min_weight_fraction_leaf=0.0, max_features='sqrt', \n",
    "                                max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                                bootstrap=True, oob_score=False, n_jobs= 4, random_state=seed, \n",
    "                                verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
    "    print('best mcc of', maxval, 'with an mtry of', bestmetrics)\n",
    "    #plt.plot(mcclist)\n",
    "\n",
    "    #plt.xlabel('mtry')\n",
    "    #plt.ylabel('mcc')\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "    model= rf.fit(xtrain, ytrain)\n",
    "    preds = model.predict(xtest)\n",
    "    results = metriccalc(preds, ytest)\n",
    "    print('for dataset', index[loops], 'validation metrics of:')\n",
    "    print('positives in data', results[0])\n",
    "    print('negatives in data', results[1], '\\n')\n",
    "    print('fn count =', results[2])\n",
    "    print('tn count =', results[3])\n",
    "    print('tp count =', results[4])\n",
    "    print('fp count =', results[5], '\\n')\n",
    "    print('net accuracy =', results[6])\n",
    "    print('positive accuracy =', results[7])\n",
    "    print('negative accuracy =', results[8], '\\n')\n",
    "    print('fpr =', results[9])\n",
    "    print('tpr =', results[10], '\\n')\n",
    "    print('f1 score =',results[11])\n",
    "    print('mcc =',results[12])\n",
    "    print('cohen Kappa =',results[13], '\\n \\n')\n",
    "    loops = loops + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "75acc5b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
